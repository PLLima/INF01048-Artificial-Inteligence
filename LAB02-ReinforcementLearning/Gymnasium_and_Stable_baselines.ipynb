{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoxOjIlOImwx"
   },
   "source": [
    "# Gymnasium - criando um MDP\n",
    "\n",
    "Baseado no \"Stable Baselines3 Tutorial - Creating a custom Gym environment\" - https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb\n",
    "\n",
    "## Introducão\n",
    "\n",
    "Este notebook é uma introdução à interface Gym para ambientes de aprendizado por reforço. A biblioteca Gymnasium fornece uma interface que permite a modelagem do ambiente de uma forma padronizada para avaliação e teste de algoritmos de aprendizado por reforço.\n",
    "\n",
    "Neste notebook, veremos como usar um algoritmo já pronto em um ambiente novo.\n",
    "\n",
    "## Instalação do Stable Baselines3 (incluindo gymnasium) usando Pip\n",
    "\n",
    "A célula abaixo instala no ambiente de execução do Colab a biblioteca Stable Baselines3 de algoritmos de RL. Uma das dependências é a gymnasium, que fornece a interface padronizada de ambientes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rugepGHWXBc0",
    "outputId": "45cd760e-ece4-4c9c-d498-ba221c98c248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3>=2.0.0a4 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (2.4.0a11)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4) (1.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.20 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4) (3.1.0)\n",
      "Requirement already satisfied: pandas in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a4) (3.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3>=2.0.0a4) (0.0.4)\n",
      "Requirement already satisfied: filelock in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3>=2.0.0a4) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3>=2.0.0a4) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from matplotlib->stable-baselines3>=2.0.0a4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from pandas->stable-baselines3>=2.0.0a4) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from pandas->stable-baselines3>=2.0.0a4) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3>=2.0.0a4) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13->stable-baselines3>=2.0.0a4) (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"stable-baselines3>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqxatIwPOXe_"
   },
   "source": [
    "##  Formato do ambiente\n",
    "\n",
    "Um MDP é composto por estados, ações, transição e recompensa. Para modelar um MDP no gymnasium, voce deve definir duas propriedades e os métodos da interface.\n",
    "\n",
    "### Propriedades\n",
    "\n",
    "As propriedades são:\n",
    "- `observation_space` contém que tipo de espaço gym (gym space: `Discrete`, `Box`, ...) e a forma da observação (e.g. matriz 4x3).\n",
    "- `action_space` também é um objeto tipo gym space, definindo o tipo de ação que pode ser feita.\n",
    "\n",
    "O melhor jeito de aprender sobre gym spaces é olhando o [código](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces). De qualquer forma, os principais tipos são::\n",
    "- `gym.spaces.Box`: Um espaço (possivelmente sem limites) em $R^n$. Especificamente, Box representa o produto cartesiano de n intervalos fechados. Cada intervalo pode ser do tipo [a, b], (-oo, b], [a, oo), or (-oo, oo). Por exemplo, um vetor 1D ou uma imagem podem ser descritas como Box.\n",
    "\n",
    "- `gym.spaces.Discrete`: um conjunto discreto $\\{ 0, 1, \\dots, n-1 \\}$, geralmente útil para definir ações.\n",
    "  Exemplo: se você tem duas ações (esquerda e direita), você pode representar seu espaço de ações com `Discrete(2)`, e ao implementar os métodos do ambiente você faz a ação 0 representar \"esquerda\" e a 1 representar \"direita\".\n",
    "\n",
    "\n",
    "### Métodos\n",
    "\n",
    "Devem ser implementados 3 métodos obrigatórios e um opcional:\n",
    "\n",
    "* `reset(seed)` para (re)iniciar o ambiente. Deve retornar a observação/estado inicial para o agente e um dict com informação adicional (pode ser vazio). Recebe uma semente aleatória para usar caso haja aleatoriedade. É chamado sempre que um novo episódio for começar.\n",
    "* `step(action)` recebe a ação a ser realizada no ambiente. O método deve realizar a ação e retornar uma tupla: `observation, reward, terminated, truncated, info` contendo, respectivamente, a observação (estado atingido), a recompensa recebida, se o episodio terminou por atingir um estado terminal (terminated) ou se foi interrompido (truncado, e.g. limite de tentativas atingido) e um dict com informações adicionais (pode ser vazio).\n",
    "* `close()`: se precisar \"limpar\" alguma coisa ao fechar o ambiente\n",
    "* (opcional) `render(method)`: gera uma visualização do ambiente. A função recebe o método de renderização (string) e deve gerar a visualização apropriada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIxbc7IIXL_Q"
   },
   "source": [
    "## Ambiente já existente\n",
    "\n",
    "Vamos usar um ambiente dentre os vários que implementam a interface gym. No caso, vamos usar o CartPole. Documentação [aqui](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mkApBFg-lFtI"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "import time, math, random\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAJNhvGKXXOb",
    "outputId": "3859b97f-61f6-4b0c-ce0f-72e090bfcb30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space: Discrete(2)\n",
      "Sampled action: 1\n",
      "[ 0.00590195  0.22249876 -0.03678154 -0.28973368] 1.0 False False {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Box(4,) means that it is a Vector with 4 components\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "# Discrete(2) means that there is two discrete actions\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# The reset method is called at the beginning of an episode\n",
    "obs = env.reset()\n",
    "# Sample a random action\n",
    "action = env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "obs, reward, truncated, terminated, info = env.step(action)\n",
    "# Note the obs is a numpy array\n",
    "# info is an empty dict for now but can contain any debugging info\n",
    "# reward is a scalar\n",
    "print(obs, reward, truncated, terminated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYNwBsrsXToU"
   },
   "source": [
    "### Executando um agente no ambiente\n",
    "\n",
    "Uma vez que o ambiente segue a interface gym, é bem facil plugar qualquer algoritmo do stable-baselines.\n",
    "\n",
    "Utilizaremos aqui o **Deep Q-learning**, utilizando uma rede **neural multilayer perceptron** para aproximar funções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9hkMCMwXp5t",
    "outputId": "82ff348f-447f-427f-f390-0c665672164c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x76c3c4422870>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Train the agent\n",
    "#model = PPO('MlpPolicy', env, verbose=1).learn(5000)\n",
    "# Crie e treine o agente PPO\n",
    "model = DQN(\"MlpPolicy\", env, verbose=0)\n",
    "model.learn(total_timesteps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s94ytvhmX452"
   },
   "source": [
    "### Avaliação do agente treinado\n",
    "A celula abaixo verifica a execução do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5HJz6_bX9cb",
    "outputId": "ab625996-9bda-4a44-d661-e4fb949aee11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pllima0909/Documents/Git/INF01048-Artificial-Inteligence/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:9.91 +/- 0.94\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "# Avalie o agente treinado\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Feche o ambiente após a renderização\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s2PfonKcSol"
   },
   "source": [
    "## Criando um novo ambiente\n",
    "\n",
    "Abaixo há uma implementação de um ambiente extremamente simples. É um grid unidimensional onde o agente começa na posição mais à direita e \"vence\" se chegar na posição mais à esquerda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rYzDXA9vJfz1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium.spaces as spaces\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Environment that follows gym interface.\n",
    "  This is a simple env where the agent must learn to go always left.\n",
    "  \"\"\"\n",
    "  # Usando renderização console, em vez de GUI ('human' render mode)\n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Define constants for clearer code\n",
    "  LEFT = 0\n",
    "  RIGHT = 1\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "\n",
    "    # Size of the 1D-grid\n",
    "    self.grid_size = grid_size\n",
    "\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "    # Example when using discrete actions, we have two: left and right\n",
    "    n_actions = 2\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "\n",
    "    # The observation will be the coordinate of the agent\n",
    "    # this can be described both by Discrete and Box space, here we use Discrete\n",
    "    # the agent can be in any position from 0 to grid_size-1\n",
    "    self.observation_space = spaces.Discrete(self.grid_size)\n",
    "\n",
    "  def reset(self, seed=None):\n",
    "    \"\"\"\n",
    "    Important: se observation_space é Box, a observação deve ser um array numpy\n",
    "    :return: Tuple[np.array, dict] (obs e info; info é sempre vazio)\n",
    "    \"\"\"\n",
    "    # Initialize the agent at the right of the grid\n",
    "    self.agent_pos = self.grid_size - 1\n",
    "    obs = self.agent_pos #np.array([self.agent_pos]) #.astype(np.uint8)\n",
    "    info = {}\n",
    "    return obs, info\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    Implementa a dinâmica do MDP. No nosso caso, basta ajustar a posição do agente\n",
    "    \"\"\"\n",
    "    if action != self.LEFT and action != self.RIGHT:    #checks for validity\n",
    "      raise ValueError(f\"Invalid action={action}\")\n",
    "\n",
    "    # position decreases if we move left, and increases if we move right\n",
    "    increment = -1 if action == self.LEFT else 1\n",
    "    self.agent_pos += increment\n",
    "\n",
    "    # Account for the boundaries of the world (keeps the agent between 0 and grid_size-1)\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size-1)\n",
    "\n",
    "    # The game \"finishes\" when the agent gets to the leftmost position\n",
    "    terminated = bool(self.agent_pos == 0)\n",
    "\n",
    "    # zero reward everywhere except when reaching the goal (left of the grid)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # Optionally we can pass additional info, we are not using that for now\n",
    "    info = {}\n",
    "\n",
    "    # converts the state to a numpy array and returns the experience tuple\n",
    "    obs = self.agent_pos\n",
    "\n",
    "    # for now, the episode has no timestep limit, so, it's never truncated\n",
    "    truncated = False\n",
    "    return obs, reward, terminated, truncated, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    \"\"\"\n",
    "    'Desenha' o ambiente para visualização\n",
    "    \"\"\"\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # agent is represented as a cross, rest as a dot\n",
    "    print(\".\" * self.agent_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size-1 - self.agent_pos))\n",
    "\n",
    "\n",
    "  def close(self):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy5mlho1-Ine"
   },
   "source": [
    "### Validar o ambiente\n",
    "\n",
    "Stable Baselines3 fornece um [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) pra verificar se o ambiente segue a interface Gym. Também checa se o ambiente é compativel com os ambientes do Stable-Baselines (e dá warning se precisar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9DOpP_B0-LXm"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = GoLeftEnv()\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ3khFtkSE0g"
   },
   "source": [
    "### Testando o ambiente com um **agente aleatorio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i62yf2LvSAYY",
    "outputId": "d733133c-8f3b-4b18-d21a-953d1f7d135b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space:  Discrete(10)\n",
      "Action space: Discrete(2)\n",
      "Initial state: obs=9. Render:                              .........x\n",
      "Step 1: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
      "Step 2: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
      "Step 3: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
      "Step 4: obs=5, rwd=0, trunc=False, term=False. Render:     .....x....\n",
      "Step 5: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
      "Step 6: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
      "Step 7: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
      "Step 8: obs=5, rwd=0, trunc=False, term=False. Render:     .....x....\n",
      "Step 9: obs=4, rwd=0, trunc=False, term=False. Render:     ....x.....\n",
      "Step 10: obs=3, rwd=0, trunc=False, term=False. Render:     ...x......\n",
      "Step 11: obs=2, rwd=0, trunc=False, term=False. Render:     ..x.......\n",
      "Step 12: obs=1, rwd=0, trunc=False, term=False. Render:     .x........\n",
      "Step 13: obs=2, rwd=0, trunc=False, term=False. Render:     ..x.......\n",
      "Step 14: obs=1, rwd=0, trunc=False, term=False. Render:     .x........\n",
      "Step 15: obs=0, rwd=1, trunc=False, term=True. Render:      x.........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "\n",
    "obs, _ = env.reset(seed=None)\n",
    "print('Obs space: ', env.observation_space)\n",
    "print('Action space:', env.action_space)\n",
    "print(f\"Initial state: obs={obs}. Render:\".ljust(58), end=' ')\n",
    "env.render()\n",
    "\n",
    "for step in range(50):\n",
    "  print(f\"Step {step + 1}:\", end=' ')\n",
    "  action = env.action_space.sample()#seleciona uma ação aleatória\n",
    "  obs, reward, term, trunc, info = env.step(action)\n",
    "  print(f'obs={obs}, rwd={reward}, trunc={trunc}, term={term}. Render:'.ljust(50), end=' ')\n",
    "  env.render()\n",
    "  if term:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv1e1qJETfHU"
   },
   "source": [
    "### Tente com algum algoritmo do Stable-Baselines\n",
    "\n",
    "Uma vez que o ambiente segue a interface gym, é bem facil plugar qualquer algoritmo do stable-baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PQfLBE28SNDr"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "env = GoLeftEnv(grid_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zRV4Q7FVUKB6"
   },
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "obs, _ = env.reset()\n",
    "model = DQN('MlpPolicy', env, verbose=0).learn(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia_AIBsEiMdb"
   },
   "source": [
    "## Teste o agente treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJbeiF0RUN-p",
    "outputId": "77e21340-c7ba-48ff-dd66-b6fa2a3eede9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: ........x.\n",
      "Step 2: .......x..\n",
      "Step 3: ......x...\n",
      "Step 4: .....x....\n",
      "Step 5: ....x.....\n",
      "Step 6: ...x......\n",
      "Step 7: ..x.......\n",
      "Step 8: .x........\n",
      "Step 9: x.........\n",
      "Goal reached! reward= 1\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "obs, _ = env.reset()\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  action, _ = model.predict(obs, deterministic=True)\n",
    "  obs, reward, term, trunc, info = env.step(action)\n",
    "  print(f\"Step {step + 1}:\", end=' ')\n",
    "  env.render(mode='console')\n",
    "  if term:\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
